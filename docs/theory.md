# Statistical Theory and Mathematical Foundations

**Synthetic Individual Patient Data (sIPD) Generation Framework**

**Author**: Ahmed Azzam, MD  
**Institution**: Department of Neuroradiology, WVU Medicine  
**Date**: January 2025

---

## Table of Contents

1. [Overview](#overview)
2. [Phase 1: Tetrachoric Correlation](#phase-1-tetrachoric-correlation)
3. [Phase 2: Iterative Proportional Fitting](#phase-2-iterative-proportional-fitting)
4. [Phase 3: Copula Theory](#phase-3-copula-theory)
5. [Phase 4: Bayesian Logistic Regression](#phase-4-bayesian-logistic-regression)
6. [Phase 5: Validation Theory](#phase-5-validation-theory)
7. [Mathematical Proofs](#mathematical-proofs)
8. [References](#references)

---

## Overview

This document provides the complete statistical and mathematical theory underlying the five-phase sIPD generation methodology. Each phase builds upon established statistical methods with rigorous theoretical foundations.

### Problem Statement

Given:
- **K studies** with 2√ó2 contingency tables for predictor-outcome relationships
- **P predictors** (binary variables)
- **Target prevalences** œÄ‚ÇÅ, œÄ‚ÇÇ, ..., œÄ‚Çö for each predictor
- **Odds ratios** OR‚ÇÅ, OR‚ÇÇ, ..., OR‚Çö from meta-analysis

Generate:
- **N synthetic patients** with binary predictors X = [X‚ÇÅ, X‚ÇÇ, ..., X‚Çö]
- **Binary outcomes** Y ‚àà {0, 1}
- Preserve correlation structure, prevalences, and outcome associations

---

## Phase 1: Tetrachoric Correlation

### Theoretical Foundation

Tetrachoric correlation assumes that observed binary variables arise from an underlying bivariate normal distribution that has been discretized by thresholding.

**Assumption**: Binary variables (X·µ¢, X‚±º) ‚àà {0,1} arise from latent continuous variables (Z·µ¢, Z‚±º) ~ N(Œº, Œ£) where:

$$
X_i = \begin{cases} 
1 & \text{if } Z_i > \tau_i \\
0 & \text{if } Z_i \leq \tau_i
\end{cases}
$$

### Mathematical Derivation

#### From 2√ó2 Table to Tetrachoric Correlation

Given 2√ó2 contingency table:

|       | Y=1 | Y=0 | Total |
|-------|-----|-----|-------|
| X=1   |  a  |  b  | a+b   |
| X=0   |  c  |  d  | c+d   |
| Total | a+c | b+d |   N   |

Define:
- œÄ‚ÇÅ = P(X=1) = (a+b)/N
- œÄ‚ÇÇ = P(Y=1) = (a+c)/N
- œÄ‚ÇÅ‚ÇÅ = P(X=1, Y=1) = a/N

The thresholds œÑ‚ÇÅ and œÑ‚ÇÇ for standard normal are:

$$
\tau_1 = \Phi^{-1}(1 - \pi_1)
$$

$$
\tau_2 = \Phi^{-1}(1 - \pi_2)
$$

The tetrachoric correlation œÅ satisfies:

$$
\pi_{11} = \Phi_2(\tau_1, \tau_2, \rho)
$$

where Œ¶‚ÇÇ is the bivariate normal CDF.

#### Estimation Method

We estimate œÅ by finding the value that satisfies:

$$
\hat{\rho} = \arg\min_\rho \left| \Phi_2(\tau_1, \tau_2, \rho) - \pi_{11} \right|
$$

This is solved numerically using Brent's method with bounds [-0.999, 0.999].

#### Properties

1. **Range**: -1 ‚â§ œÅ ‚â§ 1
2. **Interpretation**: Correlation between latent continuous variables
3. **Asymptotic Normality**: ‚àöN(œÅÃÇ - œÅ) ‚Üí N(0, V) where V depends on cell counts
4. **Consistency**: œÅÃÇ ‚Üí·µñ œÅ as N ‚Üí ‚àû

### Correlation Matrix Construction

Given P predictors, we estimate pairwise correlations to form **R** (P√óP correlation matrix).

**Challenge**: Estimated R may not be positive semi-definite due to:
- Sampling variability
- Missing data (not all pairs observed)
- Inconsistent estimates across studies

**Solution**: Project R onto nearest PSD matrix:

$$
\hat{R}_{PSD} = \arg\min_{S \succeq 0} \|R - S\|_F
$$

where ‚Äñ¬∑‚Äñ_F is Frobenius norm and S ‚™∞ 0 denotes positive semi-definite.

**Algorithm** (Higham 2002):
1. Eigen-decompose: R = QŒõQ·µÄ
2. Truncate negative eigenvalues: Œõ‚Çä = max(Œõ, Œµ¬∑I)
3. Reconstruct: RÃÇ = QŒõ‚ÇäQ·µÄ
4. Rescale diagonal to 1

---

## Phase 2: Iterative Proportional Fitting

### Theoretical Foundation

IPF (also called RAS algorithm or biproportional fitting) adjusts a multivariate distribution to match specified marginal distributions while preserving structure.

### Problem Formulation

Given:
- Initial sample Z ~ N(0, R) with R from Phase 1
- Target marginals: P(X·µ¢ = 1) = œÄ·µ¢ for i = 1, ..., P

Find transformation T: Z ‚Üí X such that:
1. X preserves correlation structure from Z
2. Marginal prevalences match targets exactly

### Algorithm

**Step 1**: Generate latent continuous variables
$$
Z \sim N(0, R)
$$

**Step 2**: Initial binary conversion
$$
X^{(0)}_i = \mathbb{1}(Z_i > \Phi^{-1}(1-\pi_i))
$$

**Step 3**: Iterative adjustment (for t = 1, 2, ..., T):
$$
X^{(t+1)} = \text{adjust\_margins}(X^{(t)}, Z, \{\pi_i\})
$$

Adjustment preserves ordering from Z while matching marginals exactly.

### Convergence Theory

**Theorem** (IPF Convergence): Under mild regularity conditions, the IPF algorithm converges to a solution XÃÇ that satisfies:

$$
\|P_{\hat{X}}(X_i = 1) - \pi_i\| < \epsilon \quad \forall i
$$

for specified tolerance Œµ.

**Proof sketch**:
1. Define Kullback-Leibler divergence: D_KL(P‚ÄñQ) = Œ£ P log(P/Q)
2. Each IPF iteration decreases D_KL
3. D_KL is strictly convex
4. Therefore, algorithm converges to global minimum

**Convergence rate**: O(1/t) where t is iteration number.

### Properties

1. **Marginal Preservation**: Exact by construction
2. **Correlation Preservation**: Approximately preserved (typically >0.95)
3. **Uniqueness**: Solution is unique under typical conditions
4. **Computational Complexity**: O(NPT) where N=samples, P=predictors, T=iterations

---

## Phase 3: Copula Theory

### Sklar's Theorem

**Theorem** (Sklar 1959): Let F be a joint distribution function with marginals F‚ÇÅ, ..., F‚Çö. Then there exists a copula C: [0,1]·¥æ ‚Üí [0,1] such that:

$$
F(x_1, ..., x_p) = C(F_1(x_1), ..., F_p(x_p))
$$

If F‚ÇÅ, ..., F‚Çö are continuous, C is unique.

### Gaussian Copula

The Gaussian copula with correlation matrix R is defined as:

$$
C_R(u_1, ..., u_p) = \Phi_R(\Phi^{-1}(u_1), ..., \Phi^{-1}(u_p))
$$

where Œ¶_R is the multivariate normal CDF with correlation R.

### Properties

1. **Tail Dependence**: Gaussian copula has zero tail dependence
2. **Symmetry**: C(u‚ÇÅ, u‚ÇÇ) = C(u‚ÇÇ, u‚ÇÅ)
3. **Fr√©chet Bounds**: W(u) ‚â§ C(u) ‚â§ M(u) for all copulas

### Application to Binary Data

For binary variables with prevalences œÄ‚ÇÅ, ..., œÄ‚Çö:

1. **Generate**: (U‚ÇÅ, ..., U‚Çö) ~ Gaussian copula with correlation R
2. **Transform**: X·µ¢ = ùüô(U·µ¢ > 1 - œÄ·µ¢)
3. **Result**: Binary variables with approximately correct correlations

### Validation

Measure correlation preservation:

$$
\rho_{preservation} = \text{cor}(\text{vec}(R_{target}), \text{vec}(R_{observed}))
$$

Typically requires œÅ_preservation > 0.95 for acceptable fit.

---

## Phase 4: Bayesian Logistic Regression

### Model Specification

The outcome Y follows a Bernoulli distribution:

$$
Y_i \sim \text{Bernoulli}(p_i)
$$

with logit link:

$$
\text{logit}(p_i) = \beta_0 + \sum_{j=1}^P \beta_j X_{ij}
$$

### Prior Distribution

Weakly informative priors on coefficients:

$$
\beta_j \sim N(\log(\text{OR}_j), \sigma^2) \quad j = 1, ..., P
$$

$$
\beta_0 \sim N(\mu_0, \sigma_0^2)
$$

where OR‚±º are published odds ratios and Œº‚ÇÄ calibrates intercept for target prevalence.

### Posterior Inference

Using Bayes' theorem:

$$
p(\beta | X, Y) \propto p(Y | X, \beta) \cdot p(\beta)
$$

Sampling via Hamiltonian Monte Carlo (HMC):
- **Chains**: 4 parallel chains
- **Warmup**: 1,000 iterations
- **Sampling**: 1,000 iterations per chain
- **Thinning**: None (HMC has low autocorrelation)

### Convergence Diagnostics

**RÃÇ statistic** (Gelman-Rubin):

$$
\hat{R} = \sqrt{\frac{\hat{V}}{W}}
$$

where:
- ≈¥ = within-chain variance
- BÃÇ = between-chain variance  
- VÃÇ = (W + B)/2

**Criterion**: RÃÇ < 1.01 for all parameters indicates convergence.

### Predictive Distribution

For new patient X*:

$$
p(Y^* = 1 | X^*, X, Y) = \int p(Y^* = 1 | X^*, \beta) \cdot p(\beta | X, Y) d\beta
$$

Approximated using posterior samples:

$$
\hat{p}(Y^* = 1 | X^*) = \frac{1}{S} \sum_{s=1}^S \text{logit}^{-1}(\beta_0^{(s)} + \sum_{j=1}^P \beta_j^{(s)} X^*_j)
$$

---

## Phase 5: Validation Theory

### Discrimination

**C-statistic** (equivalent to AUC):

$$
C = P(p_i > p_j | Y_i = 1, Y_j = 0)
$$

Interpretation: Probability that a randomly selected case has higher predicted risk than a randomly selected control.

### Calibration

**Brier Score**:

$$
BS = \frac{1}{N} \sum_{i=1}^N (Y_i - p_i)^2
$$

Lower is better; perfect calibration gives BS = 0.

**Calibration Slope**: From logistic regression of Y on logit(p):

$$
\text{logit}(Y_i) \sim \alpha + \gamma \cdot \text{logit}(p_i)
$$

Perfect calibration: Œ≥ = 1, Œ± = 0.

### Clinical Decision Rules

For CDR with predictors S ‚äÇ {1, ..., P}:

$$
\text{Recommend CT} = \bigvee_{j \in S} X_j
$$

**Sensitivity**:

$$
\text{Sens} = P(\text{CT recommended} | Y = 1)
$$

**Specificity**:

$$
\text{Spec} = P(\text{CT not recommended} | Y = 0)
$$

---

## Mathematical Proofs

### Proof 1: IPF Convergence

**Claim**: IPF algorithm converges to a solution satisfying marginal constraints.

**Proof**:
1. Define objective: minimize D_KL(P_X ‚Äñ P_Z) subject to marginal constraints
2. D_KL is strictly convex
3. Feasible set is non-empty (continuous Z can be discretized to match any marginals)
4. Each IPF step decreases D_KL monotonically
5. D_KL ‚â• 0 with minimum at 0
6. By monotone convergence theorem, algorithm converges to minimum

‚àé

### Proof 2: Tetrachoric Correlation Consistency

**Claim**: Tetrachoric correlation estimator œÅÃÇ is consistent.

**Proof**:
1. Cell proportions œÄÃÇ‚ÇÅ, œÄÃÇ‚ÇÇ, œÄÃÇ‚ÇÅ‚ÇÅ converge to true values by LLN
2. Threshold estimates œÑÃÇ‚ÇÅ, œÑÃÇ‚ÇÇ converge by continuity of Œ¶‚Åª¬π
3. Bivariate normal CDF Œ¶‚ÇÇ is continuous in œÅ
4. Therefore, solution to Œ¶‚ÇÇ(œÑÃÇ‚ÇÅ, œÑÃÇ‚ÇÇ, œÅÃÇ) = œÄÃÇ‚ÇÅ‚ÇÅ converges to solution with true parameters

‚àé

---

## References

1. **Tetrachoric Correlation**: Pearson, K. (1900). *Mathematical contributions to the theory of evolution*

2. **Iterative Proportional Fitting**: Deming & Stephan (1940). *On a least squares adjustment of a sampled frequency table*

3. **Copula Theory**: Sklar, A. (1959). *Fonctions de r√©partition √† n dimensions et leurs marges*

4. **Gaussian Copula**: Song, P. X.-K. (2000). *Multivariate dispersion models generated from Gaussian copula*

5. **Bayesian Logistic Regression**: Gelman et al. (2013). *Bayesian Data Analysis, 3rd Edition*

6. **PSD Projection**: Higham, N. J. (2002). *Computing the nearest correlation matrix*

7. **Calibration**: Steyerberg, E. W. (2009). *Clinical Prediction Models*

---

**Document Version**: 1.0  
**Last Updated**: January 2025
